{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "under-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "appreciated-teacher",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "billion-authorization",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser','tagger', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attractive-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 1198623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acquired-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "satisfactory-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = read_file('datasets/melville-moby_dick.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "perceived-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = separate_punc(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pharmaceutical-lexington",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215339"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the seques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dominant-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize into sequences of tokens\n",
    "train_len = 25+1 # 50 training words , then one target word\n",
    "\n",
    "# Empty list of sequences\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len, len(tokens)):\n",
    "    \n",
    "    # Grab train_len# amount of characters\n",
    "    seq = tokens[i-train_len:i]\n",
    "    \n",
    "    # Add to list of sequences\n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "average-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cc4bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f93e400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db01dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37ce281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to numpy array\n",
    "import numpy as np\n",
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1108faab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  159,  9473, 17255, ...,   218,   446,     5],\n",
       "       [ 9473, 17255,   406, ...,   446,     5,  1180],\n",
       "       [17255,   406,    42, ...,     5,  1180,    42],\n",
       "       ...,\n",
       "       [  240,   946,   354, ...,  1431,  1327,    74],\n",
       "       [  946,   354,  1430, ...,  1327,    74,   219],\n",
       "       [  354,  1430,     3, ...,    74,   219,   220]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25f29f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913670eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features except last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aefc50b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5dec6ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequences[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "435534f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c54bed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes =  vocabulary_size + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b699b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13491810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215313, 25)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1fdeaf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a9c61064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size, seq_len):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, seq_len, input_length=seq_len))\n",
    "    model.add(LSTM(seq_len*2, return_sequences=True))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca8162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef2341f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 25, 25)            431400    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 25, 50)            15200     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 17256)             880056    \n",
      "=================================================================\n",
      "Total params: 1,349,406\n",
      "Trainable params: 1,349,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocabulary_size+1,seq_len )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c0eefbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9bdeeed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sfm/anaconda3/envs/nlp_course/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/2\n",
      "215313/215313 [==============================] - 314s 1ms/step - loss: 7.0135 - accuracy: 0.0680\n",
      "Epoch 2/2\n",
      "215313/215313 [==============================] - 204s 947us/step - loss: 6.6059 - accuracy: 0.0840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f89027becd0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=128, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e9ba48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model \n",
    "model.save('my_moby.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "434ba20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(tokenizer, open('my_moby_tokenizer', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "56b04fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "40db12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    \n",
    "    output_text = []\n",
    "    \n",
    "    input_text = seed_text\n",
    "    \n",
    "    for i in range(num_gen_words):\n",
    "        \n",
    "        #encode the input text as we did for training\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        #pad the text.. if its more than the max lenght cut or less than the maxlen\n",
    "        pad_encoded  = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "        #pedict the word index\n",
    "        pred_word_ind =model.predict_classes(pad_encoded, verbose=0)[0]\n",
    "        #Get the predicted word\n",
    "        pred_word = tokenizer.index_word[pred_word_ind]\n",
    "        #Add to the input text for next word prediction\n",
    "        input_text += ' ' + pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "    \n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a979a4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter',\n",
       " '1',\n",
       " 'loomings',\n",
       " 'call',\n",
       " 'me',\n",
       " 'ishmael',\n",
       " 'some',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'never',\n",
       " 'mind',\n",
       " 'how',\n",
       " 'long',\n",
       " 'precisely',\n",
       " 'having',\n",
       " 'little',\n",
       " 'or',\n",
       " 'no',\n",
       " 'money',\n",
       " 'in',\n",
       " 'my',\n",
       " 'purse',\n",
       " 'and',\n",
       " 'nothing',\n",
       " 'particular',\n",
       " 'to']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dc5a9e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = ' '.join(text_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1e2c2e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chapter 1 loomings call me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "29b26dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the whale of the whale of the whale of the whale of the whale of the whale of the whale of the whale of the'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, seq_len, seed_text=seed_text, num_gen_words=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee3915e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef51a25c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
